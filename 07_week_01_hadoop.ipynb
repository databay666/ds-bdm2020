{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HADOOP ECOSYSTEM CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start the daemons again:\n",
    "\n",
    "First ssh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo service ssh stop\n",
    "sudo service ssh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service ssh status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-dfs.sh 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yarn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the job history server (so that PIG does not complain that the server does not respond):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether hdfs and yarn daemons are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of hdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfsadmin -report 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if hdfs is in safemode, stop safemode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfsadmin -safemode leave 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And start postgresql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo service postgresql start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d imdb2 -c \"\\dt+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is a data warehouse system and MapReduce wrapper that presents an SQL interface. So SQL programmers feel at home through Hive in the Hadoop Ecosystem, without writing MapReduce jobs explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start the metastore, the central repository to store metadata for hive tables:\n",
    "\n",
    "You should rerun this if the hive or pig commands return an error saying that \"thrift server does not respond\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive --service metastore &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check whether the service is listening on its port:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netstat -an | grep 9083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA WITH SQOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sqoop can import the data as Hive tables with \"--hive-import --create-hive-table\" flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First flush the directory for default import location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /user/jovyan/title_ratings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "--connect jdbc:postgresql://localhost:5432/imdb2 \\\n",
    "--username postgres \\\n",
    "--table title_ratings \\\n",
    "--hive-import --create-hive-table --direct \\\n",
    "2>&1 | grep -Pv \"INFO|WARN|SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view the imported file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -find / -name \"title*ratings\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/hive/warehouse/ 2>&1 | grep -Pv \"^(Warning|Please|WARNING)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check from hive whether files are imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'show tables' 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to read the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -cat /user/hive/warehouse/title_ratings/* 2>&1 | grep -Pv \"^SLF4J\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to delete the table, we do it from the hive command, not by manually deleting the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'drop table title_ratings' 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And cheeck again whether the table is delted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'show tables' 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -ls /user/hive/warehouse/ 2>&1 | grep -Pv \"^(Warning|Please|WARNING)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import title_ratings again for further commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r /user/jovyan/title_ratings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqoop import \\\n",
    "--connect jdbc:postgresql://localhost:5432/imdb2 \\\n",
    "--username postgres \\\n",
    "--table title_ratings \\\n",
    "--hive-import --create-hive-table --direct \\\n",
    "2>&1 | grep -Pv \"INFO|WARN|SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e 'show tables' 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A SQOOP TO HIVE IMPORT EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll import a postgresql database into hdfs as text files and hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll \n",
    "- first import a publicly available sql dump into postgresql\n",
    "- Then import the postgresql database into hdfs as text files and hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is \"World\", containing list of cities, countries and languages. The link is\n",
    "\n",
    "http://pgfoundry.org/frs/download.php/527/world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other sample databases following the links:\n",
    "\n",
    "http://pgfoundry.org/projects/dbsamples\n",
    "\n",
    "https://wiki.postgresql.org/wiki/Sample_Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget http://pgfoundry.org/frs/download.php/527/world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract the archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar -xzvf world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new database at postgresql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createdb -U postgres world2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And import the dump into the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres world2 < dbsamples-0.1/world/world.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world2 -c \"\\dt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've missed the steps, no worry. There is already a copy of the database named **world** in postgresql. We can query it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"\\dt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view the fields of the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"\\d+ public.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view some of the rows of the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from city limit 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from country limit 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql -U postgres -d world -c \"select * from countrylanguage limit 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's import into hive:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get the list of tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables=$(psql -U postgres -d world -t --pset=\"border=0\" -c \"\\dt\" | \\\n",
    "awk -F \" \" '{ print $2 }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"$tables\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a hive database to import into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"create database world\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And import each table in the postgresql database into hive database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"$tables\" | while read l;\n",
    "do\n",
    "    sqoop import \\\n",
    "    --connect jdbc:postgresql://localhost:5432/world \\\n",
    "    --username postgres \\\n",
    "    --table $l \\\n",
    "    --hive-import \\\n",
    "    --create-hive-table \\\n",
    "    --hive-table world.$l \\\n",
    "    --direct \\\n",
    "    2>&1 | grep -Pv \"INFO|WARN|SLF4J\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIVE OPERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First show tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show tables\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's create a database from hive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"create database deneme\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show databases\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete deneme database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"drop database deneme\" 2>&1 | grep -Pv \"^(Warning|Please|WARNING|SLF4J)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's run a simple query inside a hive database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"select * from title_ratings limit 1;\"  2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a hive database on hdfs, similar to our postgresql database, we can run similar queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, under the hood, Hive converts the HiveQL query to a series of map reduce jobs\n",
    "\n",
    "The plan of the conversion can be viewed by prefixing the statement with \"explain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"explain\n",
    "select * from title_ratings limit 1;\" \\\n",
    "2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is suitable for simpler queries. However as the queries get more complex and need a clearer definition of the dataflow, we should revert to a tool such as Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A HIVE EXERCISE WITH WORLD DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the tables in the world database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show tables in world\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order get information about tables, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"show create table world.city;\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will return information on the schema (column names and types), file type and size information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat it for other tables in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a very simple query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "select * from country limit 1;\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a query to see the average lifeexpantancy of all countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "select avg(lifeexpectancy) from country limit 10;\" 2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next task is:\n",
    "\n",
    "- get the names (from country table) of the countries, official languages (from country languages) of which include english "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "OK\n",
    "American Samoa\n",
    "Anguilla\n",
    "Antigua and Barbuda\n",
    "Australia\n",
    "Barbados\n",
    "Belize\n",
    "Bermuda\n",
    "United Kingdom\n",
    "Virgin Islands, British\n",
    "Cayman Islands\n",
    "South Africa\n",
    "Falkland Islands\n",
    "Gibraltar\n",
    "Guam\n",
    "Hong Kong\n",
    "Ireland\n",
    "Christmas Island\n",
    "Canada\n",
    "Cocos (Keeling) Islands\n",
    "Lesotho\n",
    "Malta\n",
    "Marshall Islands\n",
    "Montserrat\n",
    "Nauru\n",
    "Niue\n",
    "Norfolk Island\n",
    "Palau\n",
    "Northern Mariana Islands\n",
    "Saint Helena\n",
    "Saint Kitts and Nevis\n",
    "Saint Lucia\n",
    "Saint Vincent and the Grenadines\n",
    "Samoa\n",
    "Seychelles\n",
    "Tokelau\n",
    "Tonga\n",
    "Turks and Caicos Islands\n",
    "Tuvalu\n",
    "New Zealand\n",
    "Vanuatu\n",
    "United States\n",
    "Virgin Islands, U.S.\n",
    "Zimbabwe\n",
    "United States Minor Outlying Islands\n",
    "Time taken: 11.424 seconds, Fetched: 44 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive -e \"use world;\n",
    "select c.name from country c left join\n",
    "countrylanguage l on c.code=l.countrycode\n",
    "where l.isofficial = true\n",
    "and l.language = 'English';\"  2>&1 | grep -Pv \"^SLF4J\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig is a scripting language for creating workflows based on MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig transforms the declarative nature of Hive into a procedural one, so that dataflow steps are more easily defined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this examples, we will use HCatalog to connect Pig to Hive databases. HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — Pig, MapReduce — to more easily read and write data on the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order run pig commands/scripts on hive tables, first ensure that hive metastore is running and listening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netstat -an | grep 9083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And start it if it is not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive --service metastore &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want pig to use HCatalog service to connect to hive. \n",
    "\n",
    "HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — Pig, MapReduce — to more easily read and write data on the grid.\n",
    "\n",
    "Let's check whether hcat is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcat -e \"create table hcatalogtest(name string,place string,id int) row format delimited fields terminated by ':' stored as textfile\" 2>&1 | \\\n",
    "grep -Pv \"SLF4J|INFO|WARN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcat -e \"desc hcatalogtest\" 2>&1 |  grep -Pv \"SLF4J|INFO|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable pig with HCatalog:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pig -useHCatalog\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write down the steps defining the workflow and the plan will be executed when we enter the DUMP command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's load city table from world database in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will list the steps and then run them as a single script in batch mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "city = LOAD 'world.city' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's select the cities where the countrycode is TUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "cityturkey = filter city by countrycode == 'TUR';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's select the cities with a population larger than 1 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "largecitytur = filter cityturkey by population > 1000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "DUMP largecitytur;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write down all steps in to a pig script file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > ~/script1.pig <<EOF\n",
    "city = LOAD 'world.city' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "cityturkey = filter city by countrycode == 'TUR';\n",
    "largecitytur = filter cityturkey by population > 1000000;\n",
    "DUMP largecitytur;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/script1.pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And execute the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog ~/script1.pig 2>&1 | grep -Pv \"INFO|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import other tables in the world database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "\n",
    "\n",
    "lang = LOAD 'world.countrylanguage' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our previous example in Hive as a Pig dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, filter the lang table for countries, official languages of which include English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "codeen = filter lang by language == 'English' and isofficial == true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we join the filtered countrylanguage and country tables on the coeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "joinen = JOIN country by code, codeen by countrycode;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I select only the name field to be dumped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "names = foreach joinen generate name;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute the plan to dump the names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "DUMP names;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > ~/script2.pig <<EOF\n",
    "country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "lang = LOAD 'world.countrylanguage' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "codeen = filter lang by language == 'English' and isofficial == true;\n",
    "joinen = JOIN country by code, codeen by countrycode;\n",
    "names = foreach joinen generate name;\n",
    "DUMP names;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/script2.pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog ~/script2.pig 2>&1 | grep -Pv \"INFO|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIG EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as an exercise, we will compare the lifeexpentancy of the whole sample and the life expectancy of the countries with English as official language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel of calculating averages in Pig, below is the solution for the first part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "-- filter for null values\n",
    "countrynotnull = filter country by lifeexpectancy is not null;\n",
    "\n",
    "-- get lifeexpectancy column\n",
    "\n",
    "lifeall = foreach countrynotnull generate lifeexpectancy;\n",
    "\n",
    "-- combine values into a single group \n",
    "lifeallg = group lifeall all;\n",
    "\n",
    "-- calculate the average\n",
    "avgall = foreach lifeallg generate AVG(lifeall);\n",
    "-- execute\n",
    "DUMP avgall;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is:\n",
    "```Pig\n",
    "(66.486036036036)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > ~/script3.pig <<EOF\n",
    "country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "countrynotnull = filter country by lifeexpectancy is not null;\n",
    "lifeall = foreach countrynotnull generate lifeexpectancy;\n",
    "lifeallg = group lifeall all;\n",
    "avgall = foreach lifeallg generate AVG(lifeall);\n",
    "DUMP avgall;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/script3.pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog ~/script3.pig  2>&1 | grep -Pv \"INFO|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play with code in previous example to get the lifeexpectancy values of English speaking countries (Note that we had extracted the names column. Just change the column)\n",
    "\n",
    "And apply the steps above (you will have the life expectancies of anglophone countries instead of all countries, rest is the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that null elimination step is not necessary, the result is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be:\n",
    "```Pig\n",
    "(71.5027027027027)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > ~/script4.pig <<EOF\n",
    "    country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "    countrynotnull = filter country by lifeexpectancy is not null;\n",
    "    lang = LOAD 'world.countrylanguage' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "    codeen = filter lang by language == 'English' and isofficial == true;\n",
    "    joinen = JOIN countrynotnull by code, codeen by countrycode;\n",
    "    lifeen = foreach joinen generate lifeexpectancy;\n",
    "\n",
    "    lifeeng = group lifeen all;\n",
    "    avgen = foreach lifeeng generate AVG(lifeen);\n",
    "\n",
    "    DUMP avgen;\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/script4.pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig -useHCatalog ~/script4.pig  2>&1 | grep -Pv \"INFO|WARN\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
